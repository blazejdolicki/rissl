#!/bin/bash

#SBATCH -N 1 # number of nodes
#SBATCH --partition=batch
#SBATCH --job-name=evaluate_breakhis
#SBATCH --gpus-per-node=1
#SBATCH --time=00:30:00
#SBATCH --output=temp_logs/slurm_output_%j.out

source activate vissl
cd /projects/rissl/blazej/thesis/rissl/

DATA_DIR="/mnt/archive/projectdata/data_breakhis"

# supervised non-e2
#MODEL_WEIGHTS="/home/b.dolicki/logs/supervised/breakhis/14184/checkpoints/best_model.pt"
# supervised e2
#MODEL_WEIGHTS="/home/b.dolicki/logs/supervised/breakhis_e2/14240/checkpoints/best_model.pt"
# linear non-e2 no rot aug
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_no_rot_aug/14188/converted_best_model.torch"
# linear non-e2 with rot aug
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco/14176/converted_best_model.torch"
# linear e2
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14189/converted_best_model.torch"
# seed: 7
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14640/seed_7/converted_best_model.torch"
# seed: 187
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14640/seed_187/converted_best_model.torch"
# seed: 389
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14640/seed_389/converted_best_model.torch"
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14681/seed_389/converted_best_model.torch"

# finetune non-e2 no rot aug
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/finetune/finetune_breakhis_moco_no_rot_aug/14253/checkpoints/best_model.pt"
# finetune non-e2 with rot
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/finetune/finetune_breakhis_moco/14242/checkpoints/best_model.pt"
# seed 389
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/finetune/finetune_breakhis_moco/14304/checkpoints/best_model.pt"
# finetune with e2
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/finetune/finetune_breakhis_e2_moco/14256/checkpoints/best_model.pt"
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/finetune/finetune_breakhis_e2_moco/14315/checkpoints/best_model.pt"

#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2_512/14831/seed_7/converted_best_model.torch"
MODEL_WEIGHTS="/projects/rissl/blazej/logs/benchmark/linear/linear_breakhis_moco/23701/seed_666/converted_best_model.torch"
# set checkpoint dir to be inside log dir of the log dir of the loaded model
CHECKPOINT_DIR=(${MODEL_WEIGHTS//// })
unset CHECKPOINT_DIR[-1]
# go one parent dir higher if the model is trained using train.py (get out of `checkpoints` folder)
if [[ $MODEL_WEIGHTS == *"supervised"* ]] || [[ $MODEL_WEIGHTS == *"finetune"* ]]; then
  unset CHECKPOINT_DIR[-1]
fi
CHECKPOINT_DIR=/$(IFS=/ ; echo "${CHECKPOINT_DIR[*]}")/evaluate/$SLURM_JOB_ID
mkdir -p $CHECKPOINT_DIR

echo $CHECKPOINT_DIR

python evaluate.py --dataset breakhis --data_dir $DATA_DIR --split val,test --log_dir $CHECKPOINT_DIR \
                --mlflow_dir /projects/rissl/blazej/thesis/mlflow_runs \
                --checkpoint_path $MODEL_WEIGHTS \
                --batch_size=128 --num_workers=1 # --last_hid_dims=256

echo "finished"

# move slurm output from default location to logs after the job is finished
mv /projects/rissl/blazej/thesis/rissl/temp_logs/slurm_output_$SLURM_JOB_ID.out $CHECKPOINT_DIR/