#!/bin/bash

#SBATCH -N 1 # number of nodes
#SBATCH --partition=batch
#SBATCH --job-name=evaluate_breakhis
#SBATCH --gpus-per-node=1
#SBATCH --time=00:30:00
#SBATCH --output=temp_logs/slurm_output_%j.out

source activate vissl
cd $HOME/thesis/rissl/

DATA_DIR="/mnt/archive/projectdata/data_breakhis"

# supervised non-e2
MODEL_WEIGHTS="/home/b.dolicki/logs/supervised/breakhis/14184/checkpoints/best_model.pt"

# linear non-e2 no rot aug
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_no_rot_aug/14188/converted_best_model.torch"
# linear non-e2 with rot aug
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco/14176/converted_best_model.torch"
# linear e2
#MODEL_WEIGHTS="/home/b.dolicki/logs/benchmark/linear/linear_breakhis_moco_e2/14189/converted_best_model.torch"

# set checkpoint dir to be inside log dir of the log dir of the loaded model
CHECKPOINT_DIR=(${MODEL_WEIGHTS//// })
unset CHECKPOINT_DIR[-1]
# go one parent dir higher if the model is trained from scratch (get out of `checkpoints` folder)
if [[ $MODEL_WEIGHTS == *"supervised"* ]]; then
  unset CHECKPOINT_DIR[-1]
fi
CHECKPOINT_DIR=/$(IFS=/ ; echo "${CHECKPOINT_DIR[*]}")/evaluate/$SLURM_JOB_ID
mkdir -p $CHECKPOINT_DIR

python evaluate.py --dataset breakhis --data_dir $DATA_DIR --split val,test --log_dir $CHECKPOINT_DIR \
                --mlflow_dir /home/b.dolicki/thesis/mlflow_runs \
                --checkpoint_path $MODEL_WEIGHTS \
                --batch_size=512 --num_workers=1

echo "finished"

# move slurm output from default location to logs after the job is finished
mv $HOME/thesis/rissl/temp_logs/slurm_output_$SLURM_JOB_ID.out $CHECKPOINT_DIR/