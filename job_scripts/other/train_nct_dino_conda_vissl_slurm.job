#!/bin/bash


NUM_WORKERS=2
NUM_GPUS=1
NUM_TASKS=1
NUM_MACHINES=1
TRAIN=train/
SOURCE=$HOME/thesis/hissl
SINGULARITYIMAGE=$HOME/thesis/hissl_20210922_np121_h5py.sif
CONFIG_PATH=dummy/quick_gpu_resnet50_simclr
LOGS_DIR=hissl-logs
EXPERIMENT_DIR=$HOME/thesis/$LOGS_DIR
EXPERIMENT_DIR_CONTAINER=/$LOGS_DIR
DATA_ROOT=$HOME"/thesis/ssl-histo/data/NCT-CRC-HE-100K"


module load 2021
module load Anaconda3/2021.05
source activate vissl

cd $SOURCE

# for multi-machine GPUs: stops the job in case of NCCL ASYNC errors
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=INFO
#export NCCL_SOCKET_IFNAME=lo

# to silence this error:
# "ERROR: ld.so: object '/sara/tools/xalt/xalt/lib64/libxalt_init.so'
# from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored."
unset LD_PRELOAD

# we need to change NUM_CPU_PER_PROC and MEM_GB because the defaults are not possible for this Lisa partition
# default NUM_CPU_PER_PROC is 8 and default MEM_GB is 250
python3 tools/run_distributed_engines.py \
    hydra.verbose=true \
    config=$CONFIG_PATH\
    config.DATA.TRAIN.DATA_SOURCES=[synthetic] \
    config.DATA.TRAIN.DATA_LIMIT=1000 \
    config.DATA.TRAIN.BATCHSIZE_PER_REPLICA=10 \
    config.CHECKPOINT.DIR=$HOME/thesis/$EXPERIMENT_DIR_CONTAINER/$SLURM_JOB_NAME/checkpoints/$SLURM_JOB_ID \
    config.DISTRIBUTED.NUM_NODES=2 \
    config.DISTRIBUTED.NUM_PROC_PER_NODE=4 \
    config.SLURM.USE_SLURM=true \
    config.SLURM.PARTITION=gpu_titanrtx_short \
    config.SLURM.NAME=train_simclr_multinode \
    config.SLURM.TIME_HOURS=1 \
    config.SLURM.LOG_FOLDER=ssl-histo/job_logs \
    config.SLURM.NUM_CPU_PER_PROC=4 \
    config.SLURM.MEM_GB=30
