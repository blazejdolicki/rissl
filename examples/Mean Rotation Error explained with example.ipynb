{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aab85c0",
   "metadata": {},
   "source": [
    "# Mean Rotation Error explained with example\n",
    "In this notebook we compute Mean Rotation Error step by step and show intermediate results in order to verify that the implementation is correct and to improve reader's understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626795ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.029520Z",
     "start_time": "2022-06-14T13:07:23.473925Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc93ec",
   "metadata": {},
   "source": [
    "Both here and in `evaluation_mre.py` we use PyTorch's notation when possible:\n",
    "* $B$ denotes batch size\n",
    "* $C$ denotes number of input channels ($C=3$ for RGB)\n",
    "* $H$ and $W$ denote height and width of the image, respectively\n",
    "Furthermore, $N$ denotes number of rotations we use on every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b563779a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.045425Z",
     "start_time": "2022-06-14T13:07:32.031421Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "B = 5\n",
    "C = 3 # RGB\n",
    "\n",
    "# dataset specific\n",
    "H = 96\n",
    "W = 96\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2cddf",
   "metadata": {},
   "source": [
    "Let's create a dummy dataset that returns an image with all values being the same number. Because of that, the image is not affected by rotations (if the rotations are perfect and don't require interpolation i.e. $N=2$ or $N=4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a18407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.108235Z",
     "start_time": "2022-06-14T13:07:32.046424Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dummy dataset for testing an demonstration.\n",
    "        If rotation_invariant=True, it returns an image with all values equal to its index, so the images are not\n",
    "        affected by rotations, and a random label.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_height, img_width, num_channels, num_classes, rotation_invariant=True, size=10):\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.rotation_invariant = rotation_invariant\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.rotation_invariant:\n",
    "            img = torch.ones((self.num_channels, self.img_height, self.img_width)) * idx\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        label = random.randint(0, self.num_classes-1)\n",
    "        \n",
    "        return img, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbdd3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.123854Z",
     "start_time": "2022-06-14T13:07:32.109234Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DummyDataset(H, W, C, num_classes, rotation_invariant=True)\n",
    "dataloader = DataLoader(dataset, batch_size=B, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db472863",
   "metadata": {},
   "source": [
    "The operations below are repeated for every batch sampled from the dataset. Let's sample a single dummy batch with `inputs` of shape (B, C, H, W) to demonstrate MRE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f906b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.294164Z",
     "start_time": "2022-06-14T13:07:32.125839Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8645cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.309402Z",
     "start_time": "2022-06-14T13:07:32.296167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 96, 96])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ad6e2",
   "metadata": {},
   "source": [
    "The first image contains only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209c1dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.419173Z",
     "start_time": "2022-06-14T13:07:32.311358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964737fb",
   "metadata": {},
   "source": [
    "The second image contains only ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d89ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:32.464704Z",
     "start_time": "2022-06-14T13:07:32.420177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b748c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:33.012537Z",
     "start_time": "2022-06-14T13:07:33.006056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0dad6",
   "metadata": {},
   "source": [
    "Next we have to rotate the input image by $N$ angles, we do that using a custom transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef37831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:34.022416Z",
     "start_time": "2022-06-14T13:07:34.014382Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import rotate\n",
    "\n",
    "\n",
    "class DiscreteRotation:\n",
    "    \"\"\"Rotate image by one of the given angles.\n",
    "\n",
    "    Arguments:\n",
    "        angles: list(ints). List of integer degrees to pick from. E.g. [0, 90, 180, 270] for a random 90-degree-like rotation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = self.angles[torch.randperm(len(self.angles))[0]]\n",
    "        return rotate(x, angle)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(angles={self.angles})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98832da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:34.992241Z",
     "start_time": "2022-06-14T13:07:34.973401Z"
    }
   },
   "outputs": [],
   "source": [
    "angles = np.linspace(start=0, stop=360, num=N, endpoint=False)\n",
    "rotations = [DiscreteRotation(angles=[angle]) for angle in angles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa551e",
   "metadata": {},
   "source": [
    "Given $N=4$, we will rotate every image by 0, 90, 180 and 270 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4572c3fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:35.874524Z",
     "start_time": "2022-06-14T13:07:35.858715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  90., 180., 270.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da45281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:36.314551Z",
     "start_time": "2022-06-14T13:07:36.220241Z"
    }
   },
   "outputs": [],
   "source": [
    "rotated_inputs_list = [rotation(inputs) for rotation in rotations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b961c3",
   "metadata": {},
   "source": [
    "This yields a list of $N$ tensors of shape (B, C, H, W), one for every rotation. We concatenate this list into a single ($N*B$, $C$, $H$, $W$) tensor in order to pass it all at once through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6d748c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:36.844396Z",
     "start_time": "2022-06-14T13:07:36.835830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 3, 96, 96]),\n",
       " torch.Size([5, 3, 96, 96]),\n",
       " torch.Size([5, 3, 96, 96]),\n",
       " torch.Size([5, 3, 96, 96])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rot_inputs.shape for rot_inputs in rotated_inputs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7541eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:37.185219Z",
     "start_time": "2022-06-14T13:07:37.177610Z"
    }
   },
   "outputs": [],
   "source": [
    "rotated_inputs = torch.cat(rotated_inputs_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ac2d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:37.589314Z",
     "start_time": "2022-06-14T13:07:37.585209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 96, 96])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10474e37",
   "metadata": {},
   "source": [
    "It is important to note that the first $B$ indices in the 0th dimension relate to rotations by 0 degrees of each image. So the 0th index relates to the first image rotated by 0 degrees, 1st index relates the second image rotated by 0 degrees and the Bth inde relates again to the first image but rotated by 90 degrees (for $N=4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d99f3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:38.188433Z",
     "start_time": "2022-06-14T13:07:38.172904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_inputs[0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc1964a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:38.643271Z",
     "start_time": "2022-06-14T13:07:38.629513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_inputs[1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288ea044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:39.016413Z",
     "start_time": "2022-06-14T13:07:39.008976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_inputs[B,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b78bb9",
   "metadata": {},
   "source": [
    "Now let's pass these rotated inputs through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb759f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:40.034211Z",
     "start_time": "2022-06-14T13:07:39.888763Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(num_classes=num_classes)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac5f5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:40.371592Z",
     "start_time": "2022-06-14T13:07:40.184352Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(rotated_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "236649d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:40.606757Z",
     "start_time": "2022-06-14T13:07:40.587979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7405, -0.6077],\n",
       "        [ 0.3959, -0.2147],\n",
       "        [ 0.5530, -0.3086],\n",
       "        [ 0.4586, -0.3955],\n",
       "        [ 1.2360, -0.5067],\n",
       "        [ 0.7405, -0.6077],\n",
       "        [ 0.3959, -0.2147],\n",
       "        [ 0.5530, -0.3086],\n",
       "        [ 0.4586, -0.3955],\n",
       "        [ 1.2360, -0.5067],\n",
       "        [ 0.7405, -0.6077],\n",
       "        [ 0.3959, -0.2147],\n",
       "        [ 0.5530, -0.3086],\n",
       "        [ 0.4586, -0.3955],\n",
       "        [ 1.2360, -0.5067],\n",
       "        [ 0.7405, -0.6077],\n",
       "        [ 0.3959, -0.2147],\n",
       "        [ 0.5530, -0.3086],\n",
       "        [ 0.4586, -0.3955],\n",
       "        [ 1.2360, -0.5067]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0ae2c",
   "metadata": {},
   "source": [
    "The model itself doesn't include a softmax layer because it is included inside `torch.nn.CrossEntropyLoss()` during normal training. Therefore, we need to pass the outputs through a softmax to obtain probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011e7a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:41.340058Z",
     "start_time": "2022-06-14T13:07:41.323493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 2]),\n",
       " tensor([[0.7938, 0.2062],\n",
       "         [0.6481, 0.3519],\n",
       "         [0.7030, 0.2970],\n",
       "         [0.7014, 0.2986],\n",
       "         [0.8510, 0.1490],\n",
       "         [0.7938, 0.2062],\n",
       "         [0.6481, 0.3519],\n",
       "         [0.7030, 0.2970],\n",
       "         [0.7014, 0.2986],\n",
       "         [0.8510, 0.1490],\n",
       "         [0.7938, 0.2062],\n",
       "         [0.6481, 0.3519],\n",
       "         [0.7030, 0.2970],\n",
       "         [0.7014, 0.2986],\n",
       "         [0.8510, 0.1490],\n",
       "         [0.7938, 0.2062],\n",
       "         [0.6481, 0.3519],\n",
       "         [0.7030, 0.2970],\n",
       "         [0.7014, 0.2986],\n",
       "         [0.8510, 0.1490]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "probs.shape, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67bac7",
   "metadata": {},
   "source": [
    "You can see now that every row contains two probabilities that sum to 1. Another thing we observe is that every row is repeated $N$ times. That's because our images are rotation-invariant and as a consquence so are their model outputs. To make this more clear, we reshape to (B, N, num_classes) in order to have separate dimensions for images and rotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dad167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:42.094332Z",
     "start_time": "2022-06-14T13:07:42.085715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 2]),\n",
       " tensor([[[0.7938, 0.2062],\n",
       "          [0.7938, 0.2062],\n",
       "          [0.7938, 0.2062],\n",
       "          [0.7938, 0.2062]],\n",
       " \n",
       "         [[0.6481, 0.3519],\n",
       "          [0.6481, 0.3519],\n",
       "          [0.6481, 0.3519],\n",
       "          [0.6481, 0.3519]],\n",
       " \n",
       "         [[0.7030, 0.2970],\n",
       "          [0.7030, 0.2970],\n",
       "          [0.7030, 0.2970],\n",
       "          [0.7030, 0.2970]],\n",
       " \n",
       "         [[0.7014, 0.2986],\n",
       "          [0.7014, 0.2986],\n",
       "          [0.7014, 0.2986],\n",
       "          [0.7014, 0.2986]],\n",
       " \n",
       "         [[0.8510, 0.1490],\n",
       "          [0.8510, 0.1490],\n",
       "          [0.8510, 0.1490],\n",
       "          [0.8510, 0.1490]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = probs.reshape((N, B, -1)).permute(1, 0, 2)\n",
    "probs.shape, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f4aa8",
   "metadata": {},
   "source": [
    "Next, we want to keep the probabilities of the correct class for every image. We do that with `torch.gather()` which requires us to reshape our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4d035ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:43.621203Z",
     "start_time": "2022-06-14T13:07:43.600110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 1]),\n",
       " tensor([[[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices = labels.unsqueeze(dim=1).unsqueeze(dim=2).repeat(1, N, 1)\n",
    "label_indices.shape, label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78cc8515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:45.016238Z",
     "start_time": "2022-06-14T13:07:45.006427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 1]),\n",
       " tensor([[[0.2062],\n",
       "          [0.2062],\n",
       "          [0.2062],\n",
       "          [0.2062]],\n",
       " \n",
       "         [[0.3519],\n",
       "          [0.3519],\n",
       "          [0.3519],\n",
       "          [0.3519]],\n",
       " \n",
       "         [[0.2970],\n",
       "          [0.2970],\n",
       "          [0.2970],\n",
       "          [0.2970]],\n",
       " \n",
       "         [[0.7014],\n",
       "          [0.7014],\n",
       "          [0.7014],\n",
       "          [0.7014]],\n",
       " \n",
       "         [[0.1490],\n",
       "          [0.1490],\n",
       "          [0.1490],\n",
       "          [0.1490]]], grad_fn=<GatherBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_probs = torch.gather(probs, 2, label_indices)\n",
    "target_probs.shape, target_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11e66e",
   "metadata": {},
   "source": [
    "Let's print again the initial `labels`. Looking at them and at `probs` you can see that `target_probs` preserved only the probabilities of the correct class. Having done that, the compute standard deviation between rotations for every image which shows how robust the predictions are to rotations. Given that our dummy images are rotation-invariant, we practically obtain zeros for every image (although for some it's not *exactly* due to machine precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6ec127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:07:46.237849Z",
     "start_time": "2022-06-14T13:07:46.221748Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]),\n",
       " tensor([[0.0000e+00],\n",
       "         [0.0000e+00],\n",
       "         [0.0000e+00],\n",
       "         [0.0000e+00],\n",
       "         [2.2352e-08]], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = torch.std(target_probs, dim=1)\n",
    "stds.shape, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce3535",
   "metadata": {},
   "source": [
    "This confirms that our metric implementation works as expected. Here we only demonstrate the operations for one batch, but in the actual implementation, the standard deviations are added to a list and once the loop iterated over all batches, the final MRE is calculated by averaging all standard deviations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
